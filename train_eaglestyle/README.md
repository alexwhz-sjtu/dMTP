# MTP Head Training Pipeline

åŸºäº `pipeline.md` çš„æ‰©æ•£å¤´è®­ç»ƒå®Œæ•´å®ç°ã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
train_eaglestyle/
â”œâ”€â”€ README.md                           # æœ¬æ–‡ä»¶
â”œâ”€â”€ TRAINING_GUIDE.md                   # è¯¦ç»†è®­ç»ƒæŒ‡å—
â”œâ”€â”€ pipeline.md                         # åŸå§‹è®­ç»ƒæ€è·¯æ–‡æ¡£
â”œâ”€â”€ quick_start.sh                      # å¿«é€Ÿå¯åŠ¨è„šæœ¬
â”œâ”€â”€ config_examples.py                  # é…ç½®ç¤ºä¾‹
â”‚
â”œâ”€â”€ data_collection.py                  # Step 1: æ•°æ®æ”¶é›†è„šæœ¬
â”œâ”€â”€ mtp_dataset.py                      # Step 2: æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ train_mtp_head.py                   # Step 3: ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_pipeline.py                    # æµç¨‹æµ‹è¯•è„šæœ¬
â”‚
â”œâ”€â”€ mtphead_trainer.py                  # è‡ªå®šä¹‰ Trainerï¼ˆç¦»æ•£æ‰©æ•£è®­ç»ƒï¼‰
â””â”€â”€ schedulers/                         # Alpha è°ƒåº¦å™¨
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ alpha.py                        # çº¿æ€§ Alpha è°ƒåº¦å™¨
    â””â”€â”€ kappa.py                        # Kappa Alpha è°ƒåº¦å™¨
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 0. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š
```bash
pip install transformers torch tqdm
```

### 1. æ•°æ®æ”¶é›† (Data Collection)

ä»åŸºç¡€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­æ”¶é›†éšè—çŠ¶æ€å’Œ tokenï¼š

```bash
python data_collection.py \
    --base_model_path Qwen/Qwen2-7B \
    --input_data_path /path/to/text/file.txt \
    --output_dir ./mtp_collected_data \
    --max_samples 50000 \
    --sample_size 512 \
    --stride 256
```

**è¾“å…¥æ ¼å¼**ï¼šæ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªæ–‡æ¡£

**è¾“å‡º**ï¼š
```
./mtp_collected_data/collected_data_final/
â”œâ”€â”€ hidden_states.pt       # [num_samples, seq_len, hidden_size]
â”œâ”€â”€ tokens.pt              # [num_samples, seq_len]
â”œâ”€â”€ input_ids.pt           # [num_samples, seq_len]
â””â”€â”€ sample_ids.pt          # [num_samples]
```

### 2. æµ‹è¯•æµç¨‹ (Optional)

éªŒè¯æ•°æ®åŠ è½½å’Œè®­ç»ƒæµç¨‹ï¼š

```bash
python test_pipeline.py \
    --data_dir ./mtp_collected_data/collected_data_final \
    --block_length 4
```

### 3. è®­ç»ƒ MTP å¤´

```bash
python train_mtp_head.py \
    --train_data_dir ./mtp_collected_data/collected_data_final \
    --output_dir ./mtp_checkpoint \
    --block_length 4 \
    --per_device_train_batch_size 32 \
    --num_train_epochs 5 \
    --learning_rate 2e-4 \
    --warmup_steps 1000 \
    --logging_steps 50 \
    --save_steps 500
```

## ğŸ“Š è®­ç»ƒæµç¨‹è¯¦è§£

### Step 1: æ•°æ®æ”¶é›† (`data_collection.py`)

**ç›®çš„**ï¼šä»åŸºç¡€ LLM çš„æ¨ç†è¿‡ç¨‹ä¸­æ”¶é›†æ¡ä»¶éšè—çŠ¶æ€å’Œé¢„æµ‹ token

**å·¥ä½œæµ**ï¼š
1. åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
2. å¯¹æ¯ä¸ªè¾“å…¥æ–‡æœ¬è¿›è¡Œæ¨ç†
3. è®°å½•ï¼š
   - `h_l`: æ¯ä¸ªä½ç½®çš„éšè—çŠ¶æ€ï¼ˆbase model æœ€åä¸€å±‚ï¼‰
   - `t_l`: æ¯ä¸ªä½ç½®çš„é¢„æµ‹ tokenï¼ˆgreedy è§£ç ï¼‰
4. ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ›å»ºè®­ç»ƒæ ·æœ¬
5. åˆ†å—ä¿å­˜æ•°æ®

**å…³é”®å‚æ•°**ï¼š
- `sample_size`: è®­ç»ƒæ ·æœ¬é•¿åº¦ï¼ˆæ¨è 512ï¼‰
- `stride`: æ»‘åŠ¨çª—å£æ­¥é•¿ï¼ˆæ¨è 256ï¼‰
- `max_length`: å•æ¬¡æ¨ç†çš„æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ 2048ï¼‰

### Step 2: æ•°æ®åŠ è½½ (`mtp_dataset.py`)

**ç›®çš„**ï¼šå°†æ”¶é›†çš„æ•°æ®è½¬æ¢ä¸º MTP å¤´è®­ç»ƒæ ·æœ¬

**æ ·æœ¬æ ¼å¼**ï¼š
- `in_hidden_states`: æ¡ä»¶éšè—çŠ¶æ€ `h_l` [hidden_size]
- `input_ids`: æ©ç  token åºåˆ— [block_length]ï¼ˆå…¨ä¸º MASK token IDï¼‰
- `labels`: ç›®æ ‡ token `t_l, t_(l+1), ..., t_(l+L-1)` [block_length]
- `attention_mask`: æ³¨æ„åŠ›æ©ç  [1 + block_length]

**æ•°æ®æµ**ï¼š
```
åŸå§‹éšè—çŠ¶æ€å’Œ token
    â†“
MTPHeadDatasetï¼ˆæ¯ä¸ªæ ·æœ¬åŒ…å«æ¡ä»¶å’Œç›®æ ‡ï¼‰
    â†“
MTPHeadCollatorï¼ˆæ‰¹å¤„ç†ï¼‰
    â†“
DataLoaderï¼ˆè®­ç»ƒï¼‰
```

### Step 3: æ¨¡å‹è®­ç»ƒ (`train_mtp_head.py`)

**ç›®çš„**ï¼šä½¿ç”¨ç¦»æ•£æ‰©æ•£åŸç†è®­ç»ƒ MTP å¤´ï¼Œé¢„æµ‹å¤šä¸ª token

**è®­ç»ƒæµç¨‹**ï¼ˆåœ¨ `mtphead_trainer.py` çš„ `compute_loss` ä¸­å®ç°ï¼‰ï¼š

1. **æ—¶é—´é‡‡æ ·**: éšæœºé‡‡æ · $t \in [\epsilon, 1)$
2. **æ©ç æ¦‚ç‡è®¡ç®—**: $p_{mask} = 1 - \alpha(t)$ï¼Œå…¶ä¸­ $\alpha(t)$ æ¥è‡ªè°ƒåº¦å™¨
3. **éšæœºæ©ç **: ç‹¬ç«‹åœ°ä»¥æ¦‚ç‡ $p_{mask}$ æ©ç åŒ–æ¯ä¸ª tokenï¼ˆå·²æ©ç çš„ input_idsï¼‰
4. **å‰å‘ä¼ æ’­**: å°†æ©ç  token å’Œæ¡ä»¶éšè—çŠ¶æ€è¾“å…¥ MTP å¤´
5. **æŸå¤±è®¡ç®—**: äº¤å‰ç†µæŸå¤±ï¼Œä»…åœ¨æ©ç ä½ç½®è®¡ç®—
6. **åŠ æƒ**: æŒ‰è°ƒåº¦å™¨æƒé‡åŠ æƒï¼ˆå¯é€‰ï¼‰
7. **åå‘ä¼ æ’­**: æ›´æ–°æ¨¡å‹å‚æ•°

**å…³é”®å‚æ•°**ï¼š
- `block_length`: é¢„æµ‹çš„ token æ•°ï¼ˆLï¼Œæ¨è 4-8ï¼‰
- `scheduler_type`: Alpha è°ƒåº¦å™¨ç±»å‹ï¼ˆ'linear' æˆ– 'kappa'ï¼‰
- `time_epsilon`: æœ€å°æ—¶é—´æ­¥ï¼Œé¿å…é€€åŒ–ï¼ˆæ¨è 0.01ï¼‰
- `loss_weight_type`: æŸå¤±æƒé‡è®¡ç®—ï¼ˆ'scheduler' æˆ– 'ones'ï¼‰

## âš™ï¸ é…ç½®é€‰é¡¹

### å¿«é€Ÿæµ‹è¯•é…ç½®
```bash
python train_mtp_head.py \
    --train_data_dir ./data \
    --output_dir ./test_ckpt \
    --per_device_train_batch_size 8 \
    --num_train_epochs 1 \
    --logging_steps 5
```

### æ ‡å‡†è®­ç»ƒé…ç½®
```bash
python train_mtp_head.py \
    --train_data_dir ./data \
    --output_dir ./checkpoint \
    --block_length 4 \
    --per_device_train_batch_size 32 \
    --num_train_epochs 5 \
    --learning_rate 2e-4 \
    --warmup_steps 1000 \
    --scheduler_type linear
```

### å¤§è§„æ¨¡è®­ç»ƒé…ç½®
```bash
python -m torch.distributed.launch --nproc_per_node 8 train_mtp_head.py \
    --train_data_dir ./data \
    --output_dir ./large_ckpt \
    --block_length 8 \
    --per_device_train_batch_size 64 \
    --gradient_accumulation_steps 4 \
    --num_train_epochs 10
```

æŸ¥çœ‹ `config_examples.py` è·å–æ›´å¤šé¢„è®¾é…ç½®ã€‚

## ğŸ“ æ–‡ä»¶è¯¦ç»†è¯´æ˜

### data_collection.py
**ç±»**ï¼š
- `DataCollector`: ä»åŸºç¡€æ¨¡å‹æ”¶é›†æ•°æ®

**æ–¹æ³•**ï¼š
- `collect_from_text()`: ä»å•ä¸ªæ–‡æœ¬æ”¶é›†
- `collect_from_file()`: ä»æ–‡ä»¶æ‰¹é‡æ”¶é›†
- `_save_data()`: ä¿å­˜ä¸º PyTorch å¼ é‡

**è¾“å…¥**ï¼š
- åŸºç¡€æ¨¡å‹è·¯å¾„
- æ–‡æœ¬æ–‡ä»¶ï¼ˆä¸€è¡Œä¸€ä¸ªæ–‡æ¡£ï¼‰

**è¾“å‡º**ï¼š
- PyTorch å¼ é‡ï¼ˆhidden_states, tokens, input_ids, sample_idsï¼‰

---

### mtp_dataset.py
**ç±»**ï¼š
- `MTPHeadDataset`: PyTorch Dataset
- `MTPHeadCollator`: æ‰¹å¤„ç†å™¨

**å‡½æ•°**ï¼š
- `create_dataloaders()`: åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯ DataLoader

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    "in_hidden_states": [hidden_size],
    "input_ids": [block_length],
    "labels": [block_length],
    "attention_mask": [1 + block_length],
}
```

---

### train_mtp_head.py
**ç±»**ï¼š
- `ModelArguments`: æ¨¡å‹é…ç½®å‚æ•°
- `DataArguments`: æ•°æ®é…ç½®å‚æ•°
- `TrainingExtraArguments`: é¢å¤–è®­ç»ƒå‚æ•°
- `LoggingCallback`: æ—¥å¿—å›è°ƒ

**å‡½æ•°**ï¼š
- `setup_model()`: åˆå§‹åŒ–æ¨¡å‹
- `main()`: è®­ç»ƒå…¥å£

**è¾“å‡º**ï¼š
- è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆHuggingFace æ ¼å¼ï¼‰
- è®­ç»ƒä¿¡æ¯ JSON æ–‡ä»¶
- æ£€æŸ¥ç‚¹æ–‡ä»¶

---

### test_pipeline.py
**æµ‹è¯•å‡½æ•°**ï¼š
- `test_dataset_loading()`: éªŒè¯æ•°æ®åŠ è½½
- `test_model_initialization()`: éªŒè¯æ¨¡å‹åˆå§‹åŒ–
- `test_forward_pass()`: éªŒè¯å‰å‘ä¼ æ’­
- `test_training_step()`: éªŒè¯è®­ç»ƒæ­¥éª¤

---

### mtphead_trainer.py
**ç»§æ‰¿è‡ª HuggingFace Trainer**

**æ ¸å¿ƒæ–¹æ³•**ï¼š
- `compute_loss()`: å®ç°ç¦»æ•£æ‰©æ•£æŸå¤±è®¡ç®—
  - æ—¶é—´é‡‡æ ·
  - éšæœºæ©ç 
  - å‰å‘ä¼ æ’­
  - åŠ æƒäº¤å‰ç†µ

---

### schedulers/
**Alpha è°ƒåº¦å™¨**ï¼šå†³å®šæ—¶é—´æ­¥ $t$ å¯¹åº”çš„æ©ç ç‡ $\alpha(t)$

- `LinearAlphaScheduler`: çº¿æ€§è¡°å‡
- `KappaAlphaScheduler`: åŸºäº kappa å‚æ•°çš„è¡°å‡

## ğŸ” å¸¸è§é—®é¢˜

### Q: æ•°æ®æ”¶é›†å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
**A**ï¼š
- å‡å° `max_samples`
- å¢åŠ  `stride`ï¼ˆå‡å°‘æ¯ä¸ªæ–‡æœ¬çš„æ ·æœ¬æ•°ï¼‰
- ä½¿ç”¨æ›´å°çš„ `max_length`
- ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼ˆéœ€è¦ä¿®æ”¹è„šæœ¬ï¼‰

### Q: è®­ç»ƒæ˜¾å­˜ä¸è¶³ï¼Ÿ
**A**ï¼š
- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- ä½¿ç”¨ DeepSpeed Zero-2 æˆ– Zero-3
- å‡å° `block_length`

### Q: æŸå¤±ä¸ä¸‹é™ï¼Ÿ
**A**ï¼š
- æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®ï¼ˆéªŒè¯ shape å’Œå€¼ï¼‰
- å°è¯•å¢åŠ  `warmup_steps`
- å°è¯•ä¸åŒçš„ `learning_rate`
- æ£€æŸ¥ `time_epsilon` æ˜¯å¦åˆç†

### Q: å¦‚ä½•ä½¿ç”¨å¤š GPUï¼Ÿ
**A**ï¼š
```bash
python -m torch.distributed.launch --nproc_per_node 4 train_mtp_head.py ...
```

æˆ–ä½¿ç”¨ DeepSpeedï¼š
```bash
deepspeed train_mtp_head.py --deepspeed ds_config.json ...
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Simple and Effective Masked Diffusion Language Models**
   - https://arxiv.org/abs/2406.07524
   - ç¦»æ•£æ‰©æ•£è®­ç»ƒçš„ç†è®ºåŸºç¡€

2. **Large Language Diffusion Models**
   - https://arxiv.org/abs/2502.09992
   - æ‰©æ•£åœ¨ LLM ä¸­çš„åº”ç”¨

## ğŸ’¾ æ£€æŸ¥æ¸…å•

- [ ] å‡†å¤‡è¾“å…¥æ–‡æœ¬æ–‡ä»¶
- [ ] éªŒè¯åŸºç¡€æ¨¡å‹å¯æ­£å¸¸åŠ è½½
- [ ] è¿è¡Œæ•°æ®æ”¶é›†è„šæœ¬
- [ ] éªŒè¯æ”¶é›†çš„æ•°æ®å½¢çŠ¶å’Œå¤§å°
- [ ] è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯æµç¨‹
- [ ] é…ç½®è®­ç»ƒå‚æ•°
- [ ] è¿è¡Œè®­ç»ƒè„šæœ¬
- [ ] ç›‘æ§æŸå¤±æ›²çº¿
- [ ] éªŒè¯æ£€æŸ¥ç‚¹æ–‡ä»¶
- [ ] è¯„ä¼°æ¨¡å‹æ€§èƒ½

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æå‡ºé—®é¢˜å’Œæ”¹è¿›å»ºè®®ï¼

## ğŸ“„ è®¸å¯è¯

å‚è€ƒåŸºç¡€æ¨¡å‹å’Œç›¸å…³è®ºæ–‡çš„è®¸å¯è¯ã€‚
